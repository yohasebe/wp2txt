#!/usr/bin/env ruby

# frozen_string_literal: true

require_relative "../lib/wp2txt"
require_relative "../lib/wp2txt/utils"
require_relative "../lib/wp2txt/version"
require_relative "../lib/wp2txt/cli"
require_relative "../lib/wp2txt/multistream"

require "etc"
require "json"
require "optimist"
require "parallel"
require "pastel"
require "tty-spinner"

class WpApp
  include Wp2txt

  # Maximum number of processors that can be used for parallel processing
  MAX_PROCESSORS = 8
  # Debug mode flag
  DEBUG_MODE = false

  def initialize
    @pastel = Pastel.new
  end

  private

  # Calculate the number of processes to be used for parallel processing
  def calculate_num_processes(opts)
    num_processors = Etc.nprocessors
    if opts[:num_procs] && opts[:num_procs].to_i <= num_processors && opts[:num_procs].to_i <= MAX_PROCESSORS
      opts[:num_procs]
    else
      minus2 = num_processors - 2
      minus2 < MAX_PROCESSORS ? minus2 : MAX_PROCESSORS
    end.tap { |n| n = 1 if n < 1 }
  end

  # Format article based on configuration and output format
  def format_article(article, config)
    article.title = format_wiki(article.title, config)

    if config[:format] == :json
      format_article_json(article, config)
    else
      format_article_text(article, config)
    end
  end

  # Format article as JSON hash
  def format_article_json(article, config)
    result = { "title" => article.title }

    # Categories
    if config[:category]
      result["categories"] = article.categories.flatten
    else
      result["categories"] = nil
    end

    # Text content
    if config[:category_only]
      result["text"] = nil
    else
      text = build_text_content(article, config)
      result["text"] = text.strip
    end

    # Redirect
    redirect_target = extract_redirect(article)
    result["redirect"] = redirect_target

    result
  end

  # Extract redirect target from article if it's a redirect
  def extract_redirect(article)
    article.elements.each do |type, content|
      if type == :mw_redirect
        match = content.match(REDIRECT_REGEX)
        return match[1] if match
      end
    end
    nil
  end

  # Format article as text string
  def format_article_text(article, config)
    if config[:category_only]
      format_category_only(article)
    elsif config[:category] && !article.categories.empty?
      format_with_categories(article, config)
    else
      format_full_article(article, config)
    end
  end

  # Build text content from article elements
  def build_text_content(article, config)
    contents = +""
    article.elements.each do |e|
      line = process_element(e, config)
      contents << line if line
    end
    # Apply cleanup to remove leftover markup, normalize whitespace, etc.
    cleanup(contents)
  end

  # Format article with only category information (text format)
  def format_category_only(article)
    title = "#{article.title}\t"
    contents = article.categories.join(", ")
    contents << "\n"
    title + contents
  end

  # Format article with categories (includes body text)
  def format_with_categories(article, config)
    title = "\n[[#{article.title}]]\n\n"
    contents = build_text_content(article, config)

    # Add categories at the end
    contents << "\nCATEGORIES: "
    contents << article.categories.join(", ")
    contents << "\n\n"

    config[:title] ? title + contents : contents
  end

  # Format full article content
  def format_full_article(article, config)
    title = "\n[[#{article.title}]]\n\n"
    contents = build_text_content(article, config)

    config[:title] ? title + contents : contents
  end

  # Process individual element of the article
  def process_element(element, config)
    type, content = element
    case type
    when :mw_heading
      return nil if config[:summary_only]
      return nil unless config[:heading]

      content = format_wiki(content, config)
      content += "+HEADING+" if DEBUG_MODE
      content + "\n"
    when :mw_paragraph
      content = format_wiki(content, config)
      content += "+PARAGRAPH+" if DEBUG_MODE
      content + "\n"
    when :mw_table, :mw_htable
      return nil unless config[:table]

      content += "+TABLE+" if DEBUG_MODE
      content + "\n"
    when :mw_pre
      return nil unless config[:pre]

      content += "+PRE+" if DEBUG_MODE
      content + "\n"
    when :mw_quote
      content += "+QUOTE+" if DEBUG_MODE
      content + "\n"
    when :mw_unordered, :mw_ordered, :mw_definition
      return nil unless config[:list]

      content += "+LIST+" if DEBUG_MODE
      content + "\n"
    when :mw_ml_template
      return nil unless config[:multiline]

      content += "+MLTEMPLATE+" if DEBUG_MODE
      content + "\n"
    when :mw_link
      content = format_wiki(content, config)
      return nil if content.strip.empty?

      content += "+LINK+" if DEBUG_MODE
      content + "\n"
    when :mw_ml_link
      content = format_wiki(content, config)
      return nil if content.strip.empty?

      content += "+MLLINK+" if DEBUG_MODE
      content + "\n"
    when :mw_redirect
      return nil unless config[:redirect]

      content += "+REDIRECT+" if DEBUG_MODE
      content + "\n\n"
    when :mw_isolated_template
      return nil unless config[:multiline]

      content += "+ISOLATED_TEMPLATE+" if DEBUG_MODE
      content + "\n"
    when :mw_isolated_tag
      nil
    else
      return nil unless DEBUG_MODE

      content += "+OTHER+"
      content + "\n"
    end
  end

  # Process articles using streaming (new architecture)
  def process_stream(input_path, output_dir, config)
    num_processes = config[:num_procs]
    file_size_mb = config[:file_size]
    format = config[:format]
    bz2_gem = config[:bz2_gem]

    # Determine base name for output files
    base_name = File.basename(input_path, ".*")
    base_name = base_name.sub(/\.xml$/, "") # Handle .xml.bz2

    # Create stream processor
    stream = StreamProcessor.new(input_path, bz2_gem: bz2_gem)

    # Create output writer
    writer = OutputWriter.new(
      output_dir: output_dir,
      base_name: base_name,
      format: format,
      file_size_mb: file_size_mb
    )

    # Collect pages for parallel processing
    pages = []
    page_count = 0

    puts @pastel.red.bold("Processing")
    puts "Input: #{@pastel.bold(input_path)}"
    puts "Output format: #{@pastel.bold(format.to_s)}"
    puts "CPU cores: #{@pastel.bold(num_processes.to_s)}"

    # Initialize progress spinner
    spinner = TTY::Spinner.new(
      "[:spinner] Processing articles... :title",
      format: :dots,
      hide_cursor: true
    )
    spinner.auto_spin

    time_start = Time.now

    # Process in batches for memory efficiency
    batch_size = num_processes * 100
    strip_tmarker = !config[:marker]

    stream.each_page do |title, text|
      pages << [title, text]
      page_count += 1

      # Update spinner with current article
      spinner.update(title: "(#{page_count} articles)")

      # Process batch when full
      next unless pages.size >= batch_size

      process_batch(pages, writer, config, strip_tmarker, num_processes)
      pages.clear
    end

    # Process remaining pages
    process_batch(pages, writer, config, strip_tmarker, num_processes) unless pages.empty?

    # Close output
    output_files = writer.close

    time_elapsed = Time.now - time_start
    spinner.success("Done!")

    puts "\nProcessed #{@pastel.bold(page_count.to_s)} articles in #{@pastel.bold(format_time(time_elapsed))}"
    puts "Output files: #{output_files.size}"
    output_files.each { |f| puts "  - #{f}" }
  end

  # Process a batch of pages in parallel
  def process_batch(pages, writer, config, strip_tmarker, num_processes)
    results = Parallel.map(pages, in_processes: num_processes) do |title, text|
      article = Article.new(text, title, strip_tmarker)
      format_article(article, config)
    end

    results.each do |result|
      writer.write(result)
    end
  end

  # Format elapsed time
  def format_time(seconds)
    if seconds < 60
      format("%.1f seconds", seconds)
    elsif seconds < 3600
      minutes = (seconds / 60).to_i
      secs = (seconds % 60).to_i
      "#{minutes}m #{secs}s"
    else
      hours = (seconds / 3600).to_i
      minutes = ((seconds % 3600) / 60).to_i
      "#{hours}h #{minutes}m"
    end
  end

  # Parse --markers option value
  # "all" -> true (all markers enabled)
  # "none" -> false (no markers)
  # "math,code,chem" -> [:math, :code, :chem]
  def parse_markers_option(value)
    case value.to_s.downcase.strip
    when "all", "true", ""
      true
    when "none", "false"
      false
    else
      # Parse comma-separated list
      value.split(",").map { |m| m.strip.downcase.to_sym }.select do |m|
        Wp2txt::MARKER_TYPES.include?(m)
      end
    end
  end

  public

  # Main execution method
  def run
    # Parse command line options using CLI module
    opts = Wp2txt::CLI.parse_options(ARGV)

    # Handle cache operations
    if opts[:cache_status]
      show_cache_status(opts[:cache_dir])
      return
    end

    if opts[:cache_clear]
      clear_cache(opts[:cache_dir], opts[:lang])
      return
    end

    # Determine input source
    if opts[:articles] && opts[:lang]
      # Article extraction mode
      extract_specific_articles(opts)
      return
    end

    input_path = if opts[:lang]
                   download_dump(opts[:lang], opts[:cache_dir])
                 else
                   opts[:input]
                 end

    # Validate format option
    format = opts[:format].to_s.downcase.to_sym

    # Show deprecation warnings
    if opts[:convert_given] || opts[:del_interfile_given]
      puts @pastel.yellow("Note: --convert and --del-interfile options are deprecated and will be ignored.")
      puts @pastel.yellow("      Intermediate files are no longer created in v2.0+")
      puts
    end

    num_processes = calculate_num_processes(opts)

    # Build configuration hash from options
    config = {
      format: format,
      num_procs: num_processes,
      file_size: opts[:file_size],
      bz2_gem: opts[:bz2_gem]
    }

    %i[title list heading table redirect multiline category category_only
       summary_only marker extract_citations].each do |opt|
      config[opt] = opts[opt]
    end

    # Parse markers option
    config[:markers] = parse_markers_option(opts[:markers])

    # Process input
    process_stream(input_path, opts[:output_dir], config)

    puts @pastel.blue.bold("\nComplete!")
  end

  # Show cache status
  def show_cache_status(cache_dir)
    puts @pastel.cyan.bold("=== WP2TXT Cache Status ===")
    puts "Cache directory: #{cache_dir}"
    puts

    status = Wp2txt::DumpManager.all_cache_status(cache_dir)

    if status.empty?
      puts "No cached dumps found."
      return
    end

    status.each do |lang, info|
      if info[:error]
        puts "#{@pastel.yellow(lang.to_s)}: Error - #{info[:error]}"
      else
        index_size = info[:index_size] > 0 ? format_file_size(info[:index_size]) : "not downloaded"
        multistream_size = info[:multistream_size] > 0 ? format_file_size(info[:multistream_size]) : "not downloaded"
        fresh_icon = info[:fresh] ? @pastel.green("✓") : @pastel.yellow("○")

        puts "#{@pastel.bold(lang.to_s)}:"
        puts "  #{fresh_icon} Index: #{index_size}"
        puts "  #{fresh_icon} Multistream: #{multistream_size}"
        puts "  Date: #{info[:dump_date] || 'unknown'}"
        puts
      end
    end
  end

  # Clear cache
  def clear_cache(cache_dir, lang = nil)
    if lang
      puts "Clearing cache for #{lang}..."
      manager = Wp2txt::DumpManager.new(lang, cache_dir: cache_dir)
      manager.clear_cache!
      puts @pastel.green("Cache cleared for #{lang}.")
    else
      puts "Clearing all cache..."
      Wp2txt::DumpManager.clear_all_cache!(cache_dir)
      puts @pastel.green("All cache cleared.")
    end
  end

  # Download dump for a language
  def download_dump(lang, cache_dir)
    puts @pastel.cyan.bold("=== Auto-Download Mode ===")
    puts "Language: #{lang}"
    puts "Cache directory: #{cache_dir}"
    puts

    manager = Wp2txt::DumpManager.new(lang, cache_dir: cache_dir)

    # Download multistream dump (includes index download)
    puts "Checking for latest dump..."
    dump_date = manager.latest_dump_date
    puts "Latest dump date: #{dump_date}"
    puts

    # Download index and multistream
    manager.download_index
    manager.download_multistream

    puts @pastel.green("\nDownload complete!")
    puts

    # Return path to multistream file
    manager.cached_multistream_path
  end

  # Format file size for display
  def format_file_size(bytes)
    if bytes < 1024
      "#{bytes} B"
    elsif bytes < 1024 * 1024
      "#{(bytes / 1024.0).round(1)} KB"
    elsif bytes < 1024 * 1024 * 1024
      "#{(bytes / (1024.0 * 1024)).round(1)} MB"
    else
      "#{(bytes / (1024.0 * 1024 * 1024)).round(2)} GB"
    end
  end

  # Extract specific articles by title
  def extract_specific_articles(opts)
    lang = opts[:lang]
    cache_dir = opts[:cache_dir]
    article_titles = Wp2txt::CLI.parse_article_list(opts[:articles])

    puts @pastel.cyan.bold("=== Article Extraction Mode ===")
    puts "Language: #{lang}"
    puts "Articles: #{article_titles.join(', ')}"
    puts "Output: #{opts[:output_dir]}"
    puts

    # Create dump manager
    manager = Wp2txt::DumpManager.new(lang, cache_dir: cache_dir)

    # Download index only (much smaller than full dump)
    puts "Downloading index file..."
    index_path = manager.download_index
    puts

    # Load index
    puts "Loading index..."
    spinner = TTY::Spinner.new("[:spinner] Parsing index...", format: :dots, hide_cursor: true)
    spinner.auto_spin
    index = Wp2txt::MultistreamIndex.new(index_path)
    spinner.success("Done! (#{index.size} articles indexed)")
    puts

    # Find requested articles
    found_articles = []
    not_found = []

    article_titles.each do |title|
      entry = index.find_by_title(title)
      if entry
        found_articles << entry
        puts "  #{@pastel.green('✓')} Found: #{title}"
      else
        not_found << title
        puts "  #{@pastel.red('✗')} Not found: #{title}"
      end
    end

    if found_articles.empty?
      puts @pastel.red("\nNo articles found. Please check the titles.")
      return
    end

    puts

    # Group by stream offset for efficient download
    streams_needed = found_articles.map { |e| e[:offset] }.uniq.sort

    puts "Downloading #{streams_needed.size} stream(s)..."

    # Download multistream file (partial, only needed streams)
    multistream_path = download_partial_streams(manager, index, streams_needed)

    # Create multistream reader
    reader = Wp2txt::MultistreamReader.new(multistream_path, index_path)

    # Build config for processing
    format = opts[:format].to_s.downcase.to_sym
    config = build_extraction_config(opts, format)

    # Create output writer
    base_name = "#{lang}wiki_articles"
    writer = OutputWriter.new(
      output_dir: opts[:output_dir],
      base_name: base_name,
      format: format,
      file_size_mb: opts[:file_size]
    )

    # Extract and process articles
    puts "\nExtracting articles..."
    extracted_count = 0

    found_articles.each do |entry|
      title = entry[:title]
      page = reader.extract_article(title)

      if page
        article = Article.new(page[:text], page[:title], !config[:marker])
        result = format_article(article, config)
        writer.write(result)
        extracted_count += 1
        puts "  #{@pastel.green('✓')} Extracted: #{title}"
      else
        puts "  #{@pastel.yellow('!')} Could not extract: #{title}"
      end
    end

    # Close output
    output_files = writer.close

    puts
    puts @pastel.green.bold("Extraction complete!")
    puts "Extracted #{extracted_count} of #{article_titles.size} articles"

    if not_found.any?
      puts @pastel.yellow("\nNot found (#{not_found.size}):")
      not_found.each { |t| puts "  - #{t}" }
    end

    puts "\nOutput files:"
    output_files.each { |f| puts "  - #{f}" }
  end

  # Download only the streams containing the requested articles
  def download_partial_streams(manager, index, stream_offsets)
    # Calculate byte range needed
    all_offsets = index.stream_offsets
    max_offset_needed = stream_offsets.max

    # Find the end of the last needed stream
    max_idx = all_offsets.index(max_offset_needed)
    if max_idx && max_idx < all_offsets.size - 1
      end_byte = all_offsets[max_idx + 1] - 1
    else
      # Need full file for last stream
      return manager.download_multistream
    end

    # Check if we already have a suitable partial download
    stream_count = max_idx + 1
    partial_path = manager.cached_partial_multistream_path(stream_count)

    if File.exist?(partial_path)
      puts "Using cached partial download: #{File.basename(partial_path)}"
      return partial_path
    end

    # Download partial file
    manager.download_multistream(max_streams: stream_count)
  end

  # Build config hash for article extraction
  def build_extraction_config(opts, format)
    config = {
      format: format,
      num_procs: 1,  # Single-threaded for article extraction
      file_size: opts[:file_size],
      bz2_gem: opts[:bz2_gem]
    }

    %i[title list heading table redirect multiline category category_only
       summary_only marker extract_citations].each do |opt|
      config[opt] = opts[opt]
    end

    config[:markers] = parse_markers_option(opts[:markers])
    config
  end
end

# Create new instance and run
WpApp.new.run
